#!/usr/bin/env python

####
#### Usage is ./autograder
####

import sys
import re
import json

ANSWERS_FILE='expected_1.json'
SUBMISSION_FILE='submission/exercise_1.txt'
RESULTS_FILE='results/results.json'
METADATA_FILE='submission_metadata.json'

with open(ANSWERS_FILE, 'r') as f:
    expected = json.load(f)

def grade(number, file):
    number = str(number)
    file = file[file.find('(' + number + '.)'):]
    requires_letter = 'mc_answer' in expected[number]

    ##make sure they put an explanation between $$
    if('exp' in expected[number]):
        index1 = file.index('$$')
        index2 = file[index1+2:].index('$$')
        explanation = file[index1+2:index2+index1+2]
        if len(explanation.split(' ')) < 3:
            return 'Bad Explanation'

    ##check their answer between | | against expected
    if requires_letter:
        m = re.search('\|(.*?)\|', file)
        if expected[number]['mc_answer'] == m.group(1).replace(' ', '').replace('ANSWER', ''):
            return 'Correct'
        else:
            return 'Incorrect'

    return 'Correct'

def rate_limit():
    with open(METADATA_FILE) as f:
        metadata = json.load(f)
    if len(metadata['previous_submissions']) > 3:
        with open(RESULTS_FILE, 'w') as f:
            f.write(json.dumps(metadata['previous_submissions'][0]['results']))
            sys.exit()

def main():
    rate_limit()
    with open(SUBMISSION_FILE, 'r') as f:
        file = f.read()
    output = {}
    tests = []
    for i in range(1, len(expected) + 1):
        result = grade(i, file)
        if result == 'Correct':
            score = 1
        else:
            score = 0
        tests.append({ 'score': score })
    output['tests'] = tests
    with open(RESULTS_FILE, 'w') as f:
        f.write(json.dumps(output))
main()
