#!/usr/bin/env python
#!/bin/sh

####
#### Usage is ./autograder
####

import sys
import re
import json
import os
import subprocess
import signal
import time

RESULTS_FILE='results/results.json'
METADATA_FILE='submission_metadata.json'

with open(METADATA_FILE) as f:
	metadata = json.load(f)

def exceed_latency():
	if len(metadata['previous_submissions']) == 0:
		return False
	if 'latest_successful_submission_time' not in metadata['previous_submissions'][-1]['results']:
		return False
	now = metadata['created_at']
	prev = metadata['previous_submissions'][-1]['results']['latest_successful_submission_time']
	if float(now[6:7]) > float(prev[6:7]) or float(now[8:10]) > float(prev[8:10]):
		return (float(now[11:13]) + 24) * 3600 + float(now[14:16]) * 60 + float(now[17:19]) - (float(prev[11:13]) * 3600 + float(prev[14:16]) * 60 + float(prev[17:19])) < 300
	else:
		return float(now[11:13]) * 3600 + float(now[14:16]) * 60 + float(now[17:19]) - (float(prev[11:13]) * 3600 + float(prev[14:16]) * 60 + float(prev[17:19])) < 300   


def main():
	if exceed_latency():
		with open(RESULTS_FILE, 'w') as f:
			output = {}
			output['output'] = 'Note:\n'\
			'1) Your final score will be based on both autograder score and manual review of your code.\n'\
			'2) Your score is weighted against the original skeleton code.\n\n'\
			'You have submitted more than once within 5 minutes. Your last successful submission time is: ' + metadata['previous_submissions'][-1]['results']['latest_successful_submission_time'] + '. Please retry later.'
			output['submission_count'] = len(metadata['previous_submissions']) + 1
			output['latest_successful_submission_time'] = metadata['previous_submissions'][-1]['results']['latest_successful_submission_time']
			output['total_successful_submission_count'] = metadata['previous_submissions'][-1]['results']['total_successful_submission_count']
			output['tests'] = metadata['previous_submissions'][-1]['results']['tests']
			output['stdout_visibility'] = 'visible'
			f.write(json.dumps(output))
			return


	subprocess.call(["cp", "-r", "submission/.", "."])
	subprocess.call(["cp", "-r", "source/.", "."])

	output = {}
	if len(metadata['previous_submissions']) == 0:
		output['output'] = 'Note:\n'\
		'1) Your final score will be based on both autograder score and manual review of your code.\n'\
		'2) Your score is weighted against the original skeleton code.\n\n'\
		'You have unlimited submission attemps remaining.'
		output['total_successful_submission_count'] = 1
	elif 'total_successful_submission_count' not in metadata['previous_submissions'][-1]['results']:
		output['output'] = 'Note:\n'\
		'1) Your final score will be based on both autograder score and manual review of your code.\n'\
		'2) Your score is weighted against the original skeleton code.\n\n'\
		'You have unlimited submission attemps remaining.'
		output['total_successful_submission_count'] = 1
	else:
		output['output'] = 'Note:\n'\
		'1) Your final score will be based on both autograder score and manual review of your code.\n'\
		'2) Your score is weighted against the original skeleton code.\n\n'\
		'You have unlimited submission attemps remaining.'
		output['total_successful_submission_count'] = metadata['previous_submissions'][-1]['results']['total_successful_submission_count'] + 1
	
	output['stdout_visibility'] = 'visible'
	output['submission_count'] = len(metadata['previous_submissions']) + 1
	output['latest_successful_submission_time'] = metadata['created_at']

	tests = []


	
	process = subprocess.Popen(["make", "pthread-rwlock-test"], stdout=subprocess.PIPE)
	returnstr, err = process.communicate()
	if process.returncode == 1 or returnstr != '! grep -m1 pthread_rwlock *.cc *.h\n':
		print "You are not allowed to use pthread_rwlock in your code!"
		for i in range(0, 26):
			tests.append({'name': "test case " + str(i), 'score': 0, 'output': "You are not allowed to use pthread_rwlock in your code!"})
		# tests.append({'name': 'Reward for efforts', 'score': 22, 'output': "Base Score: 22"})
		tests = sorted(tests, key = lambda key: key['name'], reverse = False)
		output['tests'] = tests
		output['stdout_visibility'] = 'visible'
		with open(RESULTS_FILE, 'w') as f:
			f.write(json.dumps(output))
		return

	process = subprocess.Popen(["make", "noyield-test"], stdout=subprocess.PIPE)
	returnstr, err = process.communicate()
	if process.returncode == 1 or "Error" in returnstr:
		print "NOYIELD must not be defined for the actual submission!"
		for i in range(0, 26):
			tests.append({'name': "test case " + str(i), 'score': 0, 'output': "NOYIELD must not be defined for the actual submission!"})
		# tests.append({'name': 'Reward for efforts', 'score': 22, 'output': "Base Score: 22"})
		tests = sorted(tests, key = lambda key: key['name'], reverse = False)
		output['tests'] = tests
		output['stdout_visibility'] = 'visible'
		with open(RESULTS_FILE, 'w') as f:
			f.write(json.dumps(output))
		return

	process = subprocess.Popen(['grep -i "#ifdef P1_RWLOCK" hashchain.*'], stdout=subprocess.PIPE, shell = True)
	returnstr, err = process.communicate()
	if returnstr == '':
		print "P1_RWLOCK must be used!"
		for i in range(0, 26):
			tests.append({'name': "test case " + str(i), 'score': 0, 'output': "P1_RWLOCK must be used!"})
		# tests.append({'name': 'Reward for efforts', 'score': 22, 'output': "Base Score: 22"})
		tests = sorted(tests, key = lambda key: key['name'], reverse = False)
		output['tests'] = tests
		output['stdout_visibility'] = 'visible'
		with open(RESULTS_FILE, 'w') as f:
			f.write(json.dumps(output))
		return



	# make qcheck for individual programs
	
	# phashcoarse phashcoarserw  phashfine phashfinerw	
	

	argument = ["qcheck-phashcoarserw", "qcheck-phashfinerw"]
	total_score = 0

	for arg in argument:
		process = subprocess.Popen(["make", arg], stdout=subprocess.PIPE)
		returnstr, err = process.communicate()
		print returnstr
		lines = returnstr.split("\n")
		for i in range(0, len(lines)):
			line = lines[i].split(" ")
			if len(line) != 6:
				continue
			if line[3] == "ALL_TESTS":
				continue
			name = line[0] + " " + line[3]
			if line[5] == "PASS":
				score = 4
			else:
				score = -2
			total_score += score
			tests.append({'name': name, 'score': score, 'output': line[5]})

	if total_score == 104:
		tests.append({'name': 'weighted scoring', 'score': -4})
	if total_score <= 0:
		tests.append({'name': 'weighted scoring', 'score': -total_score + 26})
	elif total_score <= 26:
		tests.append({'name': 'weighted scoring', 'score': 26})

	tests = sorted(tests, key = lambda key: key['name'], reverse = False)
	output['tests'] = tests
	output['stdout_visibility'] = 'visible'

	with open(RESULTS_FILE, 'w') as f:
		f.write(json.dumps(output))

main()
