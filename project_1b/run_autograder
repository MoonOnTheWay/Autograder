#!/usr/bin/env python
#!/bin/sh

####
#### Usage is ./autograder
####

import sys
import re
import json
import os
import subprocess
import signal
import time

RESULTS_FILE='results/results.json'
METADATA_FILE='submission_metadata.json'

with open(METADATA_FILE) as f:
	metadata = json.load(f)

def should_rate_limit():
	if len(metadata['previous_submissions']) == 0:
		return False
	if 'total_successful_submission_count' not in metadata['previous_submissions'][-1]['results']:
		return False
	if metadata['previous_submissions'][-1]['results']['total_successful_submission_count'] >= 10:
		return True
	return False

def exceed_latency():
	if len(metadata['previous_submissions']) == 0:
		return False
	if 'latest_successful_submission_time' not in metadata['previous_submissions'][-1]['results']:
		return False
	now = metadata['created_at']
	prev = metadata['previous_submissions'][-1]['results']['latest_successful_submission_time']
	if float(now[6:7]) > float(prev[6:7]) or float(now[8:10]) > float(prev[8:10]):
		return (float(now[11:13]) + 24) * 3600 + float(now[14:16]) * 60 + float(now[17:19]) - (float(prev[11:13]) * 3600 + float(prev[14:16]) * 60 + float(prev[17:19])) < 1800
	else:
		return float(now[11:13]) * 3600 + float(now[14:16]) * 60 + float(now[17:19]) - (float(prev[11:13]) * 3600 + float(prev[14:16]) * 60 + float(prev[17:19])) < 1800   


def main():
	if should_rate_limit():
		with open(RESULTS_FILE, 'w') as f:
			output = {}
			output['output'] = 'Note:\n'\
			'1) Your final score will be based on both autograder score and manual review of your code.\n'\
			'2) Your score is weighted against the original skeleton code.\n\n'\
			'You have submitted more than 10 times. We will use your last submission as your score.'
			output['submission_count'] = len(metadata['previous_submissions']) + 1
			output['latest_successful_submission_time'] = metadata['previous_submissions'][-1]['results']['latest_successful_submission_time']
			output['total_successful_submission_count'] = metadata['previous_submissions'][-1]['results']['total_successful_submission_count']
			output['tests'] = metadata['previous_submissions'][-1]['results']['tests']
			output['stdout_visibility'] = 'visible'
			f.write(json.dumps(output))
			return 
	if exceed_latency():
		with open(RESULTS_FILE, 'w') as f:
			output = {}
			output['output'] = 'Note:\n'\
			'1) Your final score will be based on both autograder score and manual review of your code.\n'\
			'2) Your score is weighted against the original skeleton code.\n\n'\
			'You have submitted more than once within 30 minutes. Your last successful submission time is: ' + metadata['previous_submissions'][-1]['results']['latest_successful_submission_time'] + '. Please retry later.'
			output['submission_count'] = len(metadata['previous_submissions']) + 1
			output['latest_successful_submission_time'] = metadata['previous_submissions'][-1]['results']['latest_successful_submission_time']
			output['total_successful_submission_count'] = metadata['previous_submissions'][-1]['results']['total_successful_submission_count']
			output['tests'] = metadata['previous_submissions'][-1]['results']['tests']
			output['stdout_visibility'] = 'visible'
			f.write(json.dumps(output))
			return
	



	subprocess.call(["cp", "-r", "submission/.", "."])
	subprocess.call(["cp", "-r", "source/.", "."])

	output = {}
	if len(metadata['previous_submissions']) == 0:
		output['output'] = 'Note:\n'\
		'1) Your final score will be based on both autograder score and manual review of your code.\n'\
		'2) Your score is weighted against the original skeleton code.\n\n'\
		'You have 9 submission attemps remaining.'
		output['total_successful_submission_count'] = 1
	elif 'total_successful_submission_count' not in metadata['previous_submissions'][-1]['results']:
		output['output'] = 'Note:\n'\
		'1) Your final score will be based on both autograder score and manual review of your code.\n'\
		'2) Your score is weighted against the original skeleton code.\n\n'\
		'You have 9 submission attemps remaining.'
		output['total_successful_submission_count'] = 1
	else:
		output['output'] = 'Note:\n'\
		'1) Your final score will be based on both autograder score and manual review of your code.\n'\
		'2) Your score is weighted against the original skeleton code.\n\n'\
		'You have ' + str(10 - 1 - metadata['previous_submissions'][-1]['results']['total_successful_submission_count']) + ' submission attemps remaining.'
		output['total_successful_submission_count'] = metadata['previous_submissions'][-1]['results']['total_successful_submission_count'] + 1
	output['submission_count'] = len(metadata['previous_submissions']) + 1
	output['latest_successful_submission_time'] = metadata['created_at']

	tests = []

	process = subprocess.Popen("find /usr -name libc-header-start.h", stdout=subprocess.PIPE, shell = True)
	returnstr, err = process.communicate()

	process = subprocess.Popen(["make", "depend"])
	process.communicate()

	process = subprocess.Popen(["make", "qcheck"], stdout=subprocess.PIPE)
	returnstr, err = process.communicate()
	print returnstr
	lines = returnstr.split("\n")

	sem = set()
	sem.add(3)
	sem.add(6)
	sem.add(13)
	sem.add(15)
	sem.add(16)
	sem.add(17)

	lock = set()
	lock.add(3)
	lock.add(6)
	lock.add(13)
	lock.add(15)
	lock.add(16)
	lock.add(17)

	rw = set()
	rw.add(3)
	rw.add(6)
	rw.add(7)
	rw.add(8)
	rw.add(13)
	rw.add(15)
	rw.add(16)
	rw.add(17)

	
	for i in range(0, len(lines)):
		line = lines[i].split(" ")
		if len(line) != 4:
				continue
		if line[2].startswith('-'):
				continue
		name = line[0] + "-" + line[2][:len(line[2]) - 1] + ".txt"
		with open('results/' + name) as f:
			name = name[:len(name) - 4] + ": " + f.readline().split('\'')[1]
		if line[3] == "PASS":
			if line[0] == 'nachos_sem':
				if int(line[2][:len(line[2]) - 1]) in sem:
					score = 4
				else:
					score = 1
			elif line[0] == 'nachos_lock':
				if int(line[2][:len(line[2]) - 1]) in lock:
					score = 4
				else:
					score = 1
			elif line[0] == 'nachos_rw':
				if int(line[2][:len(line[2]) - 1]) in rw:
					score = 4
				else:
					score = 1
		else:
			score = 0
		tests.append({'name': name, 'score': score, 'output': line[3] + '. Score you get for this test case: ' + str(score)})

	tests = sorted(tests, key = lambda key: key['name'], reverse = False)
	output['tests'] = tests
	output['stdout_visibility'] = 'visible'

	with open(RESULTS_FILE, 'w') as f:
		f.write(json.dumps(output))

main()
